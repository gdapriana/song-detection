{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T06:43:24.957113Z",
     "start_time": "2025-01-30T06:43:24.680376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct"
   ],
   "id": "bea364962e32f1fe",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PRE-EMPHASIS",
   "id": "ab1ac58885cc7d39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pre-emphasis is an initial stage in the Mel Frequency Cepstral Coefficients (MFCC) extraction process to improve the quality of the sound signal before extracting its features. Pre-emphasis is done by applying a high-pass filter to amplify the high frequency components of the audio signal.\n",
    "\n",
    "$$y(n)=x(n)-\\alpha.x(n-1)$$\n",
    "\n",
    "- $n$ is the input signal.\n",
    "- $y(n)$ is the signal after pre-emphasis.\n",
    "- $\\alpha$ is the *pre-emphasis* coefficient which is in the range of 0 to 1. The commonly used value of *α* is about 0.95."
   ],
   "id": "4706f3a23be2977b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `func pre_emphasis__`\n",
    "> **parameter:**\n",
    "> * `signal`: input signal, np.ndarray [shape=(n,) or (…, n)]\n",
    "> * `coefficient`: pre-emphasis coefficient, float \n",
    "\n",
    "> **output:**\n",
    "> * `pre-emphasis signal`, np.ndaarray [shape=(n, ) or (…, n)]"
   ],
   "id": "68dbfe7f60f142ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T06:43:24.963510Z",
     "start_time": "2025-01-30T06:43:24.960958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_emphasis__(signal, coefficient=0.97):\n",
    "  return np.append(signal[0], signal[1:] - coefficient * signal[:-1])"
   ],
   "id": "76747158676993e5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FRAME BLOCKING",
   "id": "eb5ee264b259434e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the Frame Blocking process, the speech signal is split into many small chunks called frames, with each frame overlapping each other. This process is designed to minimize the loss of important information (deleted) or disconnected pieces of signal during frame division. This operation continues until the entire audio signal is thoroughly mapped into frames. By dividing the signal into frames, the information contained in it can be represented in a more detailed and specific manner, making it easier for sound processing algorithms to process. In addition, frame blocking also plays an important role in overcoming variations in the duration of the sound signal, making the feature extraction process more consistent and reliable for various purposes, such as speech recognition or audio analysis.\n",
    "\n",
    "$$frame = \\frac {I-N} {M} + 1$$\n",
    "\n",
    "Description:\n",
    "- $I$ is the value of *sampling rates.*\n",
    "- $N$ indicates the *size* of *frame blocking.*\n",
    "- $M$ is the length of *overlap.*"
   ],
   "id": "582a4c86b90d4577"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `func framing__`\n",
    "> **parameter**\n",
    "> * `signal`: pre-emphased signal, np.ndarray [shape=(n,) or (…, n)]\n",
    "> * `sr`: signal sampling rate, int or float\n",
    "> * `frame_size`: size of frame in second (> 1), default 1\n",
    "> * `frame_stride`: size of frame step in second (> 1), default 0.5\n",
    "\n",
    "> **output**\n",
    "> `framed signal`, 2D arrays"
   ],
   "id": "dccd27a44734c53e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T06:43:25.050185Z",
     "start_time": "2025-01-30T06:43:25.047413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def framing__(signal, sr, frame_size=1, frame_stride=0.5):\n",
    "  frame_length, frame_step = int(round(frame_size * sr)), int(round(frame_stride * sr))\n",
    "  signal_length = len(signal)\n",
    "  num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step) + 1)\n",
    "  pad_signal_length = (num_frames - 1) * frame_step + frame_length\n",
    "  z = np.zeros((pad_signal_length - signal_length))\n",
    "  pad_signal = np.append(signal, z)\n",
    "  indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step,frame_step), (frame_length, 1)).T\n",
    "  framed_signal = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "  return framed_signal"
   ],
   "id": "543706d2410c8c1f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WINDOWING",
   "id": "39b3d2153abbb2c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Every frame, a windowing process using a specific window function is performed on the sound signal that has been processed in the previous step. Windowing attempts to eliminate distortions caused by abrupt frame retrieval. To make the frames smoother and conform to the constraints of the Fourier signal, windowing divides each frame into smaller segments to minimize the influence of edges on the signal.\n",
    "\n",
    "There are several windowing techniques that can be used in MFCC, including hanning, hamming, bartlett, blackman, kaiser, and gaussian. The Hamming window function is defined by the equation\n",
    "\n",
    "$$ w(n)=0.54-0.46 * cos \\bigg (\\frac {2\\pi n} {N-1} \\bigg) $$\n",
    "\n",
    "Description:\n",
    "\n",
    "- $n$ is the sample index in *frame*,\n",
    "- $N$ is the size of *frame*\n",
    "- $w(n)$ is the value of the *Window* *Hamming* function"
   ],
   "id": "58068681c2218de5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### `func windowing__`\n",
    "> **parameter**\n",
    "> * `signal`: framed signal, 2D arrays\n",
    "> * `sr`: signal sampling rate, int or float\n",
    "> * `frame_size`: size of frame in second (> 1), default 1\n",
    "\n",
    "> **output**\n",
    "> `windowed signal`, 2D arrays\n"
   ],
   "id": "6acc827b40dcc766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def windowing__(signal, sr, frame_size=1):\n",
    "  windowed = signal * np.hamming(int(round(frame_size * sr)))\n",
    "  return windowed"
   ],
   "id": "66ece989774c36c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FFT",
   "id": "e9f599b0e3601291"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Fast Fourier Transform (FFT) is used to transform an audio signal from the time domain to the frequency domain, so that the frequency content of the signal can be analyzed. The original audio signal being in the time domain (amplitude versus time) is not sufficient to identify spectral information such as the dominant frequency. Therefore, a Fast Fourier Transform (FFT) is performed to convert the signal into a spectral representation (amplitude versus frequency).\n",
    "The Fast Fourier Transform (FFT) is defined by Eq:\n",
    "$$ y[k]=\\sum_{n=0}^{N-1}e^{-2\\pi j\\frac{kn}{N}}x[n] $$\n",
    "Description:\n",
    "\n",
    "- $y[k]$ is the representation of signal $x[n]$ in the frequency domain.\n",
    "- $e^{-2\\pi j\\frac{kn}{N}}$ is the complex exponential factor (also known as Fourier basis), which is responsible for mapping the signal from time domain to frequency domain.\n",
    "- $x[n]$ is the amplitude value of the signal at time index $n$.\n",
    "- $N$ is the signal length or the number of samples in the discrete signal $x[n]$"
   ],
   "id": "681313b87d0d76fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### `func fft__`\n",
    "> **parameter**\n",
    "> * `signal`: windowed signal, 2D arrays\n",
    "> * `NFFT`: Number of points along transformation axis in the input to use, int or float, default 512\n",
    "\n",
    "> **output**\n",
    "> `power spectrum of fft`, 2D arrays\n"
   ],
   "id": "cae4b8eba369e784"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fft__(signal, NFFT=512):\n",
    "  mag_frames = np.absolute(np.fft.rfft(signal, NFFT))\n",
    "  return ((1.0 / NFFT) * ((mag_frames) ** 2))"
   ],
   "id": "3557caad093e7a94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MEL FILTERBANK",
   "id": "67c143565ce5424a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Mel Filterbank is used to convert the linear frequency spectrum (the result of the Fourier transform) into a Mel frequency scale, which is more in line with human auditory perception. The Mel frequency scale reflects the human perception of sound frequency, which is not directly proportional to the linear frequency scale.\n",
    "Frequency to mel scale conversion is defined by the equation\n",
    "$$ m=2595 \\cdot \\log_{10} \\bigg ( 1+\\frac {f} {700} \\bigg ) $$\n",
    "\n",
    "Description:\n",
    "- $f$ is the linear frequency in *hertz.*\n",
    "- $m$ is the frequency in *Mel scale.*\n",
    "\n"
   ],
   "id": "36cc9d0d4f4bd846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### `func melbank__`\n",
    "> **parameter**\n",
    "> * `signal`: power spectrum of fft, 2D arrays\n",
    "> * `sr`: signal sampling rate, int or float\n",
    "> * `NFFT`: Number of points along transformation axis in the input to use, int, default 512\n",
    "> * `NFILT`: Number of total filter, int, default 40\n",
    "\n",
    "> **output**\n",
    "> `filtered bank signal`, 2D arrays\n"
   ],
   "id": "548331834500171e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def melbank__(signal, sr, NFFT=512, NFILT=40):\n",
    "  low_freq_mel = 0\n",
    "  high_freq_mel = (2595 * np.log10(1 + (sr / 2) / 700))\n",
    "  mel_points = np.linspace(low_freq_mel, high_freq_mel, NFILT + 2)\n",
    "  hz_points = (700 * (10**(mel_points / 2595) - 1))\n",
    "  bin = np.floor((NFFT + 1) * hz_points / sr)\n",
    "  fbank = np.zeros((NFILT, int(np.floor(NFFT / 2 + 1))))\n",
    "  for m in range(1, NFILT + 1):\n",
    "    f_m_minus = int(bin[m - 1])\n",
    "    f_m = int(bin[m])\n",
    "    f_m_plus = int(bin[m + 1])\n",
    "    for k in range(f_m_minus, f_m):\n",
    "      fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "    for k in range(f_m, f_m_plus):\n",
    "      fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "  filter_banks = np.dot(signal, fbank.T)\n",
    "  filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "  return 20 * np.log10(filter_banks)"
   ],
   "id": "49c734b5fd51898f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DCT",
   "id": "3fbade30f3104df7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DCT is used after the Mel Filterbank stage. After calculating the energy at each filter in the Mel filterbank, the next step is to apply DCT to the energy values. DCT produces a series of cepstral coefficients that represent the spectral characteristics of the sound signal. The application of DCT to the energy values of the Mel filterbank is done with the aim of reducing the data dimension and extracting important information from the frequency domain into the cepstral domain. DCT produces a series of cepstral coefficients that describe the energy distribution of the speech signal in the cepstral domain, and these coefficients are used as acoustic features for speech analysis and recognition.\n",
    "Terdapat beberapa tipe dari DCT salah satunya adalah tipe 2, didefinisikan dengan persamaan:\n",
    "$$y_{k}=2\\sum_{n=10}^{N-1}x_{n}\\cos \\bigg( \\frac {\\pi k (2n+1)} {2N} \\bigg)$$\n",
    "\n",
    "Description:\n",
    "\n",
    "- $y_k$ is the DCT Coefficient at the $k$th index.\n",
    "- $N$ is the total number of samples.\n",
    "- $x_n$ is the input data value (in the context of MFCC, this is the log energy of the mel bank filter).\n",
    "- $cos$ is the cosine function used to convert the data to the frequency domain.\n"
   ],
   "id": "784cb30398d14999"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### `func dct__`\n",
    "> **parameter**\n",
    "> * `signal`: filtered bank signal, 2D arrays\n",
    "> * `coefficient`: Number of total coefficient taken, int, default 13\n",
    "\n",
    "> **output**\n",
    "> `transformed melbank`, 2D arrays\n"
   ],
   "id": "f1974df950683958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dct__(signal, coefficient=13):\n",
    "  return dct(signal, type=2, axis=1, norm='ortho')[:, 1 : (coefficient + 1)]"
   ],
   "id": "6cc32bf25d76df09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
